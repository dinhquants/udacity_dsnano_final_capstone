# Capstone Project: Predict Churn with small data set

## üèóÔ∏è Table of contents

1. [Installation](#installation)
2. [Project Motivation](#motivation)
3. [File Descriptions](#files)
4. [Results](#results)
5. [Licensing, Authors, and Acknowledgements](#licensing)

## Installation <a name="installation"></a>

- Install PySpark
- Run the notebook `Sparkify.ipynb` on the Anaconda distribution of Python.  The code should run with no issues using Python versions 3.*.

## Project Motivation<a name="motivation"></a>

Predicting churn rates is a challenging and common situation that data scientists and analysts regularly encounter in any customer-facing business. If you can accurately identify these customers before they leave, your business can offer them discounts and incentives, potentially saving your business in revenue. Additionally, the ability to efficiently manipulate large datasets with Spark is one of the highest-demand skills in the field of data.

The full set of files related to this course are owned by Udacity, so they are not publicly available here.  However, you can see pieces of the analysis here.  This README also serves as a template for students to follow in creating their own project README files.

## File Descriptions <a name="files"></a>

There is one notebooks available here to showcase work related to the above questions.  The notebooks is exploratory in searching through the data pertaining to the questions showcased by the notebook title.  Markdown cells were used to assist in walking through the thought process for individual steps.  

## Results<a name="results"></a>

The main result of analysis dataset can be found at the blog post available [here](https://medium.com/@dinhnda2021/wisconsin-breast-cancer-dataset-analysis-2db2a51bde9d).

## Licensing, Authors, Acknowledgements<a name="licensing"></a>

This project has utilized mini subset (128MB) of the full dataset available (12GB). Optionally, you can choose to deploy a Spark cluster on the cloud using AWS or IBM Cloud to analyze a larger amount of data. Currently we have the full 12GB [dataset](http://udacity-dsnd.s3.amazonaws.com/sparkify/mini_sparkify_event_data.json) available to you if you use AWS. If you use IBM, you can download a medium sized dataset to upload to your cluster.  
